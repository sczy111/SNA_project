{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46460fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import collections\n",
    "import random\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from networkx.algorithms.community import kernighan_lin_bisection\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import label_propagation_communities\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Generate a user interaction network\n",
    "\n",
    "interactions = pd.read_csv(\"RAW_interactions.csv\")\n",
    "interactions = interactions[['user_id', 'recipe_id', 'rating']]\n",
    "\n",
    "# Due to the large volume of the dataset, we will get the first 50 unique users as an example\n",
    "selected_users = interactions['user_id'].drop_duplicates().head(50).tolist()\n",
    "\n",
    "\n",
    "filtered_interactions = interactions[interactions['user_id'].isin(selected_users)]\n",
    "recipe_user_map = defaultdict(list)\n",
    "for _, row in filtered_interactions.iterrows():\n",
    "    recipe_user_map[row['recipe_id']].append((row['user_id'], row['rating']))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(selected_users)\n",
    "\n",
    "# Create edges based on shared recipes\n",
    "edge_weights = defaultdict(int)\n",
    "for users_ratings in recipe_user_map.values():\n",
    "    for (user1, rating1), (user2, rating2) in combinations(users_ratings, 2):\n",
    "        if user1 == user2:\n",
    "            continue\n",
    "        edge = tuple(sorted((user1, user2)))\n",
    "        similarity = 1 - abs(rating1 - rating2) / 5\n",
    "        edge_weights[edge] += similarity\n",
    "\n",
    "for (user1, user2), weight in edge_weights.items():\n",
    "    if weight >= 1:\n",
    "        G.add_edge(user1, user2, weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(14, 12))\n",
    "pos = nx.shell_layout(G)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=300)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray')\n",
    "nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "plt.title(\"User Interaction Network (First 50 Users)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946c57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2 Visualize and plot the degree distribution\n",
    "#calculate centralities\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "\n",
    "degree_vals = list(degree_centrality.values())\n",
    "closeness_vals = list(closeness_centrality.values())\n",
    "betweenness_vals = list(betweenness_centrality.values())\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ax[0].hist(degree_vals, bins=10, color='skyblue', edgecolor='black')\n",
    "ax[0].set_title('Degree Centrality Distribution')\n",
    "ax[0].set_xlabel('Degree Centrality')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "ax[1].hist(closeness_vals, bins=10, color='lightgreen', edgecolor='black')\n",
    "ax[1].set_title('Closeness Centrality Distribution')\n",
    "ax[1].set_xlabel('Closeness Centrality')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "\n",
    "ax[2].hist(betweenness_vals, bins=10, color='salmon', edgecolor='black')\n",
    "ax[2].set_title('Betweenness Centrality Distribution')\n",
    "ax[2].set_xlabel('Betweenness Centrality')\n",
    "ax[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca342e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Detect communities within the user interaction network\n",
    "\n",
    "# Louvain Community Detection\n",
    "louvain_partition = community_louvain.best_partition(G)\n",
    "louvain_colors = [louvain_partition[node] for node in G.nodes()]\n",
    "\n",
    "pos = nx.shell_layout(G)\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, pos, node_color=louvain_colors, with_labels=True, node_size=300, cmap=plt.cm.Set3)\n",
    "plt.title(\"Louvain Community Detection\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Girvan-Newman Community Detection\n",
    "gn_generator = girvan_newman(G)\n",
    "top_level_communities = next(gn_generator)\n",
    "gn_communities = [list(c) for c in top_level_communities]\n",
    "\n",
    "gn_node_color_map = {}\n",
    "for i, community in enumerate(gn_communities):\n",
    "    for node in community:\n",
    "        gn_node_color_map[node] = i\n",
    "gn_colors = [gn_node_color_map[node] for node in G.nodes()]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, pos, node_color=gn_colors, with_labels=True, node_size=300, cmap=plt.cm.Set2)\n",
    "plt.title(\"Girvan-Newman Community Detection\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Community Statistics\n",
    "\n",
    "def compute_community_stats_lo(graph, partition):\n",
    "    reverse_partition = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        reverse_partition[comm_id].append(node)\n",
    "\n",
    "    stats = []\n",
    "    for comm_id, nodes in reverse_partition.items():\n",
    "        subgraph = graph.subgraph(nodes)\n",
    "        num_nodes = subgraph.number_of_nodes()\n",
    "        num_edges = subgraph.number_of_edges()\n",
    "        if num_nodes > 0:\n",
    "            avg_degree = np.mean([d for _, d in subgraph.degree()])\n",
    "        else:\n",
    "            avg_degree = 0\n",
    "\n",
    "        if nx.is_connected(subgraph) and num_nodes > 1:\n",
    "            diameter = nx.diameter(subgraph)\n",
    "        else:\n",
    "            diameter = float('nan')\n",
    "            \n",
    "        if nx.is_connected(subgraph) and num_nodes > 1:\n",
    "            avg_path = nx.average_shortest_path_length(subgraph)\n",
    "        else:\n",
    "            avg_path = float('nan')\n",
    "        stats.append({\n",
    "            'Community': comm_id,\n",
    "            'Nodes': num_nodes,\n",
    "            'Edges': num_edges,\n",
    "            'Diameter': diameter,\n",
    "            'Avg Path Length': avg_path,\n",
    "            'Avg Degree': avg_degree\n",
    "        })\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "def compute_community_stats_gn(graph, communities):\n",
    "    stats = []\n",
    "    for i, nodes in enumerate(communities):\n",
    "        subgraph = graph.subgraph(nodes)\n",
    "        num_nodes = subgraph.number_of_nodes()\n",
    "        num_edges = subgraph.number_of_edges()\n",
    "        if num_nodes > 0:\n",
    "            avg_degree = np.mean([d for _, d in subgraph.degree()])\n",
    "        else:\n",
    "            avg_degree = 0\n",
    "\n",
    "        if nx.is_connected(subgraph) and num_nodes > 1:\n",
    "            diameter = nx.diameter(subgraph)\n",
    "        else:\n",
    "            diameter = float('nan')\n",
    "            \n",
    "        if nx.is_connected(subgraph) and num_nodes > 1:\n",
    "            avg_path = nx.average_shortest_path_length(subgraph)\n",
    "        else:\n",
    "            avg_path = float('nan')\n",
    "        stats.append({\n",
    "            'Community': f'Community {i+1}',\n",
    "            'Nodes': num_nodes,\n",
    "            'Edges': num_edges,\n",
    "            'Diameter': diameter,\n",
    "            'Avg Path Length': avg_path,\n",
    "            'Avg Degree': avg_degree\n",
    "        })\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "#print the result\n",
    "community_stats_lo = compute_community_stats_lo(G, louvain_partition)\n",
    "print(\"\\nCommunity Summary (Louvain):\")\n",
    "print(community_stats_lo)\n",
    "\n",
    "community_stats_gn = compute_community_stats_gn(G, gn_communities)\n",
    "print(\"\\nCommunity Summary (Girvan-Newman):\")\n",
    "print(community_stats_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Analyze the assortativity of the network\n",
    "\n",
    "#Calculate average rating per user\n",
    "user_avg_rating = interactions.groupby('user_id')['rating'].mean().to_dict()\n",
    "\n",
    "#Compute assortativity coefficient\n",
    "nx.set_node_attributes(G, user_avg_rating, \"avg_rating\")\n",
    "assortativity = nx.numeric_assortativity_coefficient(G, \"avg_rating\")\n",
    "print(f\"Assortativity Coefficient (by avg rating): {assortativity:.4f}\")\n",
    "rating_vals = [user_avg_rating.get(n, 0) for n in G.nodes()]\n",
    "pos = nx.shell_layout(G)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_color=rating_vals, cmap=plt.cm.plasma, node_size=300)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "plt.colorbar(nodes, label=\"User Average Rating\")\n",
    "plt.title(\"User Interaction Network Colored by Average Rating\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "#Compute core number for each node\n",
    "core_nums = nx.core_number(G)\n",
    "\n",
    "\n",
    "nx.set_node_attributes(G, core_nums, \"core\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.shell_layout(G)\n",
    "node_colors = [core_nums[node] for node in G.nodes()]\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.viridis, node_size=300)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "plt.colorbar(nodes, label=\"Core Number\")\n",
    "plt.title(\"k-Core Decomposition of User Interaction Network\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#Analyze node counts per core level\n",
    "core_counts = pd.Series(list(core_nums.values())).value_counts().sort_index()\n",
    "print(\"\\nNumber of nodes per k-core:\")\n",
    "print(core_counts)\n",
    "\n",
    "core_subgraphs = {}\n",
    "core_stats = []\n",
    "\n",
    "# Analyze each k-core\n",
    "for k in sorted(set(core_nums.values())):\n",
    "    subG = nx.k_core(G, k)\n",
    "    degrees = [d for _, d in subG.degree()]\n",
    "    clustering_coeffs = list(nx.clustering(subG).values())\n",
    "    density = nx.density(subG)\n",
    "    avg_degree = np.mean(degrees) if degrees else 0\n",
    "    avg_clustering = np.mean(clustering_coeffs) if clustering_coeffs else 0\n",
    "\n",
    "    core_stats.append({\n",
    "        'k-core': k,\n",
    "        'Nodes': subG.number_of_nodes(),\n",
    "        'Edges': subG.number_of_edges(),\n",
    "        'Density': density,\n",
    "        'Avg Degree': avg_degree,\n",
    "        'Avg Clustering': avg_clustering\n",
    "    })\n",
    "\n",
    "\n",
    "core_stats_df = pd.DataFrame(core_stats)\n",
    "print(core_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344207f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "#use the fist 50 as an example\n",
    "df = pd.read_csv(\"PP_recipes.csv\", nrows=50)\n",
    "\n",
    "\n",
    "df['ingredient_tokens'] = df['ingredient_tokens'].apply(ast.literal_eval)\n",
    "df['steps_tokens'] = df['steps_tokens'].apply(ast.literal_eval)\n",
    "df['ingredient_ids'] = df['ingredient_ids'].apply(ast.literal_eval)\n",
    "df['n_ingredients'] = df['ingredient_tokens'].apply(len)\n",
    "df['n_steps'] = df['steps_tokens'].apply(len)\n",
    "df['n_ingredient_ids'] = df['ingredient_ids'].apply(len)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "ax[0].hist(df['n_ingredients'], bins=10, color='skyblue', edgecolor='black')\n",
    "ax[0].set_title('Number of Ingredient Tokens')\n",
    "ax[0].set_xlabel('Count')\n",
    "ax[0].set_ylabel('Recipes')\n",
    "\n",
    "ax[1].hist(df['n_steps'], bins=10, color='lightgreen', edgecolor='black')\n",
    "ax[1].set_title('Number of Step Tokens')\n",
    "ax[1].set_xlabel('Count')\n",
    "ax[1].set_ylabel('Recipes')\n",
    "\n",
    "ax[2].hist(df['n_ingredient_ids'], bins=10, color='salmon', edgecolor='black')\n",
    "ax[2].set_title('Number of Ingredient IDs')\n",
    "ax[2].set_xlabel('Count')\n",
    "ax[2].set_ylabel('Recipes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "\n",
    "recipes_df = pd.read_csv(\"PP_recipes.csv\", usecols=[\"id\", \"ingredient_ids\"])\n",
    "interactions_df = pd.read_csv(\"RAW_interactions.csv\", usecols=[\"user_id\", \"recipe_id\", \"rating\"])\n",
    "\n",
    "#Merge datasets on recipe_id and id\n",
    "recipes_df['ingredient_ids'] = recipes_df['ingredient_ids'].apply(ast.literal_eval)\n",
    "merged_df = interactions_df.merge(recipes_df, left_on=\"recipe_id\", right_on=\"id\")\n",
    "\n",
    "\n",
    "user_data = merged_df.groupby(\"user_id\").agg({\n",
    "    'recipe_id': list,\n",
    "    'rating': list,\n",
    "    'ingredient_ids': lambda x: [i for sublist in x for i in sublist]\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "user_data.rename(columns={\n",
    "    'recipe_id': 'rated_recipes',\n",
    "    'rating': 'rating_list',\n",
    "    'ingredient_ids': 'ingredients'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "#export\n",
    "user_data.to_csv(\"User_Data.csv\", index=False)\n",
    "\n",
    "\n",
    "user_data['num_rated'] = user_data['rated_recipes'].apply(len)\n",
    "user_data['num_ingredients'] = user_data['ingredients'].apply(len)\n",
    "user_data['avg_rating'] = user_data['rating_list'].apply(lambda x: sum(x) / len(x) if x else 0)\n",
    "\n",
    "sampled_users = user_data.head(50)\n",
    "\n",
    "# Plot distributions for first 50 users only\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "ax[0].hist(sampled_users['num_rated'], bins=10, color='skyblue', edgecolor='black')\n",
    "ax[0].set_title('Rated Recipes (First 50 Users)')\n",
    "ax[0].set_xlabel('Recipes Rated')\n",
    "ax[0].set_ylabel('Users')\n",
    "\n",
    "ax[1].hist(sampled_users['num_ingredients'], bins=10, color='lightgreen', edgecolor='black')\n",
    "ax[1].set_title('Total Ingredients (First 50 Users)')\n",
    "ax[1].set_xlabel('Total Ingredients')\n",
    "ax[1].set_ylabel('Users')\n",
    "\n",
    "ax[2].hist(sampled_users['avg_rating'], bins=10, color='salmon', edgecolor='black')\n",
    "ax[2].set_title('Average Rating (First 50 Users)')\n",
    "ax[2].set_xlabel('Avg Rating')\n",
    "ax[2].set_ylabel('Users')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "recipes_df = pd.read_csv(\"PP_recipes.csv\", usecols=[\"id\", \"ingredient_ids\"])\n",
    "recipes_df['ingredient_ids'] = recipes_df['ingredient_ids'].apply(ast.literal_eval)\n",
    "\n",
    "# Limit to a sample(100 as an example)\n",
    "recipes_sample = recipes_df.head(100)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "ingredient_matrix = mlb.fit_transform(recipes_sample['ingredient_ids'])\n",
    "\n",
    "ingredient_matrix = ingredient_matrix.astype(bool)\n",
    "jaccard_similarity = 1 - pairwise_distances(ingredient_matrix, metric=\"jaccard\")\n",
    "similarity_df = pd.DataFrame(jaccard_similarity, index=recipes_sample['id'], columns=recipes_sample['id'])\n",
    "\n",
    "#use the first receips as an example\n",
    "top_similar = similarity_df.iloc[0].sort_values(ascending=False).iloc[1:6]\n",
    "print(\"Top similar recipes to Recipe ID\", similarity_df.index[0])\n",
    "print(top_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a651b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "G = nx.Graph()\n",
    "recipe_ids = similarity_df.index.tolist()\n",
    "\n",
    "G.add_nodes_from(recipe_ids)\n",
    "\n",
    "# Add edges for similarity >= 0.2\n",
    "for i in range(len(recipe_ids)):\n",
    "    for j in range(i + 1, len(recipe_ids)):\n",
    "        sim = similarity_df.iloc[i, j]\n",
    "        if sim >= 0.2:\n",
    "            G.add_edge(recipe_ids[i], recipe_ids[j], weight=sim)\n",
    "\n",
    "# Louvain Community Detection\n",
    "louvain_partition = community_louvain.best_partition(G)\n",
    "louvain_communities = pd.Series(louvain_partition).value_counts()\n",
    "print(\"Louvain Clustering\")\n",
    "print(f\"Number of communities: {louvain_communities.count()}\")\n",
    "print(\"Sizes:\", louvain_communities.to_dict())\n",
    "\n",
    "# Girvan-Newman\n",
    "gn_communities_gen = girvan_newman(G)\n",
    "gn_top_level = next(gn_communities_gen)\n",
    "gn_communities_list = [list(c) for c in gn_top_level]\n",
    "print(\"\\nGirvan-Newman Clustering\")\n",
    "print(f\"Number of communities: {len(gn_communities_list)}\")\n",
    "print(\"Sizes:\", [len(c) for c in gn_communities_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "from collections import Counter\n",
    "recipes_df = pd.read_csv(\"PP_recipes.csv\", usecols=[\"id\", \"ingredient_ids\", \"calorie_level\"])\n",
    "recipes_df['ingredient_ids'] = recipes_df['ingredient_ids'].apply(ast.literal_eval)\n",
    "\n",
    "# Load and merge ratings\n",
    "ratings_df = pd.read_csv(\"RAW_interactions.csv\", usecols=[\"user_id\", \"recipe_id\", \"rating\"])\n",
    "recipe_ratings = ratings_df.groupby('recipe_id')['rating'].mean().reset_index()\n",
    "recipes_df = recipes_df.merge(recipe_ratings, left_on='id', right_on='recipe_id', how='left')\n",
    "recipes_df['rating'] = recipes_df['rating'].fillna(0)\n",
    "\n",
    "# Analyse Louvian\n",
    "recipes_df['cluster'] = recipes_df['id'].map(louvain_partition)\n",
    "cluster_summary = []\n",
    "for cluster_id, group in recipes_df.groupby(\"cluster\"):\n",
    "    all_ingredients = [i for lst in group['ingredient_ids'] for i in lst]\n",
    "    top_ingredients = Counter(all_ingredients).most_common(5)\n",
    "    top_cal_level = group['calorie_level'].mode().iloc[0] if not group['calorie_level'].isna().all() else 'N/A'\n",
    "    avg_rating = group['rating'].mean()\n",
    "\n",
    "    cluster_summary.append({\n",
    "        \"Cluster\": cluster_id,\n",
    "        \"Num Recipes\": len(group),\n",
    "        \"Top Ingredients\": [i for i, _ in top_ingredients],\n",
    "        \"Most Common Calorie Level\": top_cal_level,\n",
    "        \"Average Rating\": avg_rating\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_summary)\n",
    "print(cluster_df)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(cluster_df['Cluster'].astype(str), cluster_df['Average Rating'], color='lightblue')\n",
    "plt.title('Average Rating per Cluster')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "calorie_dist = recipes_df.groupby(['cluster', 'calorie_level']).size().unstack(fill_value=0)\n",
    "calorie_dist.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "plt.title('Calorie Level Distribution by Cluster')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Recipes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64468be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import pairwise_distances, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "recipes_df = pd.read_csv(\"PP_recipes.csv\", usecols=[\"id\", \"ingredient_ids\"])\n",
    "interactions_df = pd.read_csv(\"RAW_interactions.csv\", usecols=[\"user_id\", \"recipe_id\", \"rating\"])\n",
    "\n",
    "recipes_df['ingredient_ids'] = recipes_df['ingredient_ids'].apply(ast.literal_eval)\n",
    "interactions_df = interactions_df[interactions_df['recipe_id'].isin(recipes_df['id'])]  \n",
    "\n",
    "# Compute Jaccard Similarity Matrix\n",
    "recipes_sample = recipes_df.head(100).copy()\n",
    "mlb = MultiLabelBinarizer()\n",
    "ingredient_matrix = mlb.fit_transform(recipes_sample['ingredient_ids']).astype(bool)\n",
    "\n",
    "jaccard_similarity = 1 - pairwise_distances(ingredient_matrix, metric=\"jaccard\")\n",
    "similarity_df = pd.DataFrame(jaccard_similarity, index=recipes_sample['id'], columns=recipes_sample['id'])\n",
    "\n",
    "# Merge for full data and train/test split\n",
    "merged_df = interactions_df.merge(recipes_sample, left_on=\"recipe_id\", right_on=\"id\")\n",
    "user_groups = merged_df.groupby(\"user_id\")\n",
    "\n",
    "# Build train/test split\n",
    "train_rows, test_rows = [], []\n",
    "\n",
    "for user, group in user_groups:\n",
    "    if len(group) < 2:\n",
    "        continue \n",
    "    train, test = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    train_rows.append(train)\n",
    "    test_rows.append(test)\n",
    "\n",
    "train_df = pd.concat(train_rows)\n",
    "test_df = pd.concat(test_rows)\n",
    "\n",
    "# evaluate \n",
    "def predict_rating(user_id, target_recipe_id):\n",
    "    user_history = train_df[train_df['user_id'] == user_id]\n",
    "    if user_history.empty or target_recipe_id not in similarity_df.index:\n",
    "        return np.nan\n",
    "    rated_recipes = user_history['recipe_id']\n",
    "    ratings = user_history['rating']\n",
    "    sims = []\n",
    "    for recipe, rating in zip(rated_recipes, ratings):\n",
    "        if recipe in similarity_df.columns:\n",
    "            sim = similarity_df.at[target_recipe_id, recipe]\n",
    "            sims.append((sim, rating))\n",
    "    if not sims:\n",
    "        return np.nan\n",
    "    sims = sorted(sims, key=lambda x: x[0], reverse=True)\n",
    "    n = sum(sim * rating for sim, rating in sims)\n",
    "    d = sum(sim for sim, _ in sims)\n",
    "    return n / d if d != 0 else np.nan\n",
    "\n",
    "# Evaluate predictions on the test set\n",
    "true_ratings, predicted_ratings = [], []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    pred = predict_rating(row['user_id'], row['recipe_id'])\n",
    "    if not np.isnan(pred):\n",
    "        true_ratings.append(row['rating'])\n",
    "        predicted_ratings.append(pred)\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"MAE  = {mae:.4f}\")\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "\n",
    "recommendations = {}\n",
    "\n",
    "for user_id, group in train_df.groupby(\"user_id\"):\n",
    "    liked = group[group[\"rating\"] >= 4][\"recipe_id\"]\n",
    "    if liked.empty:\n",
    "        continue\n",
    "\n",
    "    rated = set(group[\"recipe_id\"])\n",
    "    sim_scores = similarity_df.loc[liked].mean(axis=0)\n",
    "    sim_scores = sim_scores[~sim_scores.index.isin(rated)]  \n",
    "    top_recs = sim_scores.sort_values(ascending=False).head(5).index.tolist()\n",
    "    recommendations[user_id] = top_recs\n",
    "\n",
    "#Show recommendation for the first 10 users\n",
    "print(\"Recommendations for the frist 10 users\")\n",
    "for user, recs in list(recommendations.items())[:10]:\n",
    "    print(f\"User {user} ->Recommended: {recs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bfa1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#12\n",
    "df = pd.read_csv(\"RAW_interactions.csv\", usecols=[\"user_id\", \"recipe_id\", \"rating\"])\n",
    "\n",
    "# Filter for active users and recipes(use the first 100 as an example)\n",
    "active_users = df['user_id'].value_counts()[lambda x: x >= 5].index[:100] # Keeps only users who rated at least 5 recipes\n",
    "active_recipes = df['recipe_id'].value_counts()[lambda x: x >= 3].index[:100]# Keeps only recipes that have been rated at least 3 times\n",
    "\n",
    "df = df[df['user_id'].isin(active_users) & df['recipe_id'].isin(active_recipes)]\n",
    "\n",
    "\n",
    "user_map = {uid: i for i, uid in enumerate(df[\"user_id\"].unique())}\n",
    "recipe_map = {rid: i for i, rid in enumerate(df[\"recipe_id\"].unique())}\n",
    "inv_user_map = {v: k for k, v in user_map.items()}\n",
    "inv_recipe_map = {v: k for k, v in recipe_map.items()}\n",
    "\n",
    "df[\"user_index\"] = df[\"user_id\"].map(user_map)\n",
    "df[\"recipe_index\"] = df[\"recipe_id\"].map(recipe_map)\n",
    "\n",
    "\n",
    "rating_matrix = df.pivot(index=\"user_index\", columns=\"recipe_index\", values=\"rating\")\n",
    "train_rows, test_rows = [], []\n",
    "for user in rating_matrix.index:\n",
    "    user_ratings = rating_matrix.loc[user].dropna()\n",
    "    if len(user_ratings) < 2:\n",
    "        continue\n",
    "    train_idx, test_idx = train_test_split(user_ratings.index, test_size=0.2, random_state=42)\n",
    "    train_rows.append(pd.Series(user_ratings[train_idx], name=user))\n",
    "    test_rows.append(pd.Series(user_ratings[test_idx], name=user))\n",
    "\n",
    "train_matrix = pd.DataFrame(train_rows).T\n",
    "test_matrix = pd.DataFrame(test_rows).T\n",
    "\n",
    "# Pearson similarity function\n",
    "def pearson_similarity(u, v):\n",
    "    common = train_matrix[u].dropna().index.intersection(train_matrix[v].dropna().index)\n",
    "    if len(common) < 1:\n",
    "        return 0\n",
    "    ur = train_matrix.loc[common, u]\n",
    "    vr = train_matrix.loc[common, v]\n",
    "    n = ((ur - ur.mean()) * (vr - vr.mean())).sum()\n",
    "    d = np.sqrt(((ur - ur.mean()) ** 2).sum()) * np.sqrt(((vr - vr.mean()) ** 2).sum())\n",
    "    return n/ d if d != 0 else 0\n",
    "\n",
    "# Predict rating\n",
    "def predict_rating(user, item):\n",
    "    if item not in train_matrix.index:\n",
    "        return np.nan  \n",
    "    numer, denom = 0, 0\n",
    "    for other in train_matrix.columns:\n",
    "        if other == user or item not in train_matrix.index or pd.isna(train_matrix.at[item, other]):\n",
    "            continue\n",
    "        sim = pearson_similarity(user, other)\n",
    "        numer += sim * train_matrix.at[item, other]\n",
    "        denom += abs(sim)\n",
    "    return numer / denom if denom > 0 else np.nan\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "true_ratings, predicted_ratings = [], []\n",
    "\n",
    "for user in test_matrix.columns:\n",
    "    for item in test_matrix[user].dropna().index:\n",
    "        actual = test_matrix.at[item, user]\n",
    "        pred = predict_rating(user, item)\n",
    "        if not np.isnan(pred):\n",
    "            true_ratings.append(actual)\n",
    "            predicted_ratings.append(pred)\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "\n",
    "#print(\"\\nCollaborative Filtering Evaluation:\")\n",
    "#print(f\"MAE  = {mae:.4f}\")\n",
    "#print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Generate top-10 recommendations\n",
    "recommendations = {}\n",
    "\n",
    "for user in train_matrix.columns:\n",
    "    rated = train_matrix[user].dropna().index\n",
    "    unrated = train_matrix.index.difference(rated)\n",
    "\n",
    "    rec_scores = {}\n",
    "    for item in unrated:\n",
    "        total, sim_sum = 0, 0\n",
    "        for other in train_matrix.columns:\n",
    "            if other == user or pd.isna(train_matrix.loc[item, other]):\n",
    "                continue\n",
    "            sim = pearson_similarity(user, other)\n",
    "            total += sim * train_matrix.loc[item, other]\n",
    "            sim_sum += abs(sim)\n",
    "        if sim_sum > 0:\n",
    "            rec_scores[item] = total / sim_sum\n",
    "\n",
    "    if rec_scores:\n",
    "        top_items = sorted(rec_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        recommendations[user] = [inv_recipe_map[i] for i, _ in top_items]\n",
    "\n",
    "print(\"Recommendations for the frist 10 users\")\n",
    "for user_idx, recs in list(recommendations.items())[:10]:\n",
    "    real_user_id = list(user_map.keys())[list(user_map.values()).index(user_idx)]\n",
    "    print(f\"User {real_user_id} -> Recommended Recipes: {recs}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13  Neural Collaborative Filtering (NCF)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess data(Use the first 5000 to increase speed)\n",
    "df = pd.read_csv(\"RAW_interactions.csv\", usecols=[\"user_id\", \"recipe_id\", \"rating\"]).head(5000)\n",
    "df = df[df['rating'].notna()]\n",
    "\n",
    "user_map = {uid: i for i, uid in enumerate(df['user_id'].unique())}\n",
    "item_map = {iid: i for i, iid in enumerate(df['recipe_id'].unique())}\n",
    "df['user'] = df['user_id'].map(user_map)\n",
    "df['item'] = df['recipe_id'].map(item_map)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Dataset\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['item'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_ds = RatingsDataset(train)\n",
    "test_ds = RatingsDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256)\n",
    "\n",
    "# NCF model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=32):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.fc1 = nn.Linear(emb_size * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_emb(users)\n",
    "        i = self.item_emb(items)\n",
    "        x = torch.cat([u, i], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x).squeeze()\n",
    "\n",
    "# Train model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NCF(len(user_map), len(item_map)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5): \n",
    "    model.train()\n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Train loss = {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in test_loader:\n",
    "        users, items = users.to(device), items.to(device)\n",
    "        preds = model(users, items).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(ratings.numpy())\n",
    "\n",
    "rmse = mean_squared_error(all_targets, all_preds, squared=False)\n",
    "print(f\"\\nNCF RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "top_n = defaultdict(list)\n",
    "item_indices = list(inv_recipe_map.keys())\n",
    "\n",
    "for user in df['user'].unique():\n",
    "    user_tensor = torch.tensor([user] * len(item_indices), dtype=torch.long).to(device)\n",
    "    item_tensor = torch.tensor(item_indices, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, item_tensor).cpu().numpy()\n",
    "\n",
    "    rated_items = set(df[df['user'] == user]['item'])\n",
    "    unrated_scores = [(i, score) for i, score in enumerate(scores) if item_indices[i] not in rated_items]\n",
    "\n",
    "    top_items = sorted(unrated_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_n[user] = [inv_recipe_map[item_indices[i]] for i, _ in top_items]\n",
    "\n",
    "# Show sample recommendations(the first 10 users)\n",
    "print(\"Recommendations for the frist 10 users\")\n",
    "for user_id, recs in list(top_n.items())[:10]:\n",
    "    real_user = list(user_map.keys())[list(user_map.values()).index(user_id)]\n",
    "    print(f\"User {real_user} â†’ Recommended Recipes: {recs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287e1f1",
   "metadata": {},
   "source": [
    "# Final Analysis\n",
    "\n",
    "## Deep Learning-Based Recommenders \n",
    "\n",
    "One of the recent state-of-the-art models is **Neural Collaborative Filtering (NCF)**\n",
    "\n",
    "\n",
    "###  Evaluation of NCF Results\n",
    "\n",
    "In our implementation of Neural Collaborative Filtering on the frist 5000 interactions:\n",
    "- **Training loss** steadily decreased from **16.30 to 5.25** over 5 epochs.\n",
    "- The model achieved a **test RMSE of 2.3437**, indicating a reasonable prediction error for the given data volume.\n",
    "\n",
    "_**Note**: due to random weight initialization and shuffling during training, the values above may vary slightly across different runs_\n",
    "\n",
    "This aligns with the original findings by He et al. (2017), which show that NCF effectively learns non-linear patterns in user-item interaction data. The declining loss demonstrates the model's convergence, and the RMSE suggests moderately accurate predictions under a limited data scenario.\n",
    "\n",
    "Additionally, as noted in related literature, neural models benefit from:\n",
    "- Longer training with more epochs\n",
    "- Larger and denser datasets\n",
    "- Enhanced user/item context through side features\n",
    "\n",
    "These enhancements can further reduce error and improve recommendation quality.\n",
    "\n",
    "**Reference**:  \n",
    "He, X. et al. (2017). *Neural Collaborative Filtering*. WWW Conference.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Limitations\n",
    "\n",
    "\n",
    "**Content-Based Filtering**:Feature limitation, struggles to introduce diverse or unexpected content \n",
    "\n",
    "**Collaborative Filtering**: Needs high user overlap, suffers from sparsity\n",
    "\n",
    "**Pearson Similarity**: Assumes linearity, sensitive to outliers\n",
    "\n",
    "**MAE / RMSE Evaluation**: Doesn't reflect ranking or user satisfaction\n",
    "\n",
    "**Small Dataset Issues** :Low overlap hurts performance and recommendations\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fb190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
